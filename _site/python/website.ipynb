{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an interactive python notebook used to populate the datasets wesbsite (https://anteromontonio.github.io/datasetsLj/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First, we need to import the python functions, we do this by loading the file ``` build_datasets```.\n",
    "Please, only modify this file if you know what you are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load build_datasets.py\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#this is a change\n",
    "\n",
    "# This is kind of the main routine, it is used to build a dataset website from a csvfile\n",
    "def build_pages_for_dataset(\n",
    "        csv_file: str, #name of the csv_file without extension\n",
    "        dataset_id: str, #inner id for the dataset, eg. \"chiralMaps\"\n",
    "        dataset_title: str, #actual title to be displayed on the website, eg. \"Chiral maps up to 6000 edges\"\n",
    "        max_rows :int =10000,\n",
    ") -> None :\n",
    "    \n",
    "    data=pd.read_csv(f\"../csv/{csv_file}.csv\")#reads csv file\n",
    "    preprocess_DataSet(data)\n",
    "    \n",
    "    if not os.path.exists(f'../{dataset_id}'):\n",
    "        os.mkdir(f'../{dataset_id}')\n",
    "\n",
    "    build_tables_from_csv(data,max_rows,dataset_id,dataset_title)\n",
    "    if not os.path.exists(f'../_datasets/{dataset_id}.md'): #only runs if the file does not exists, to avoid overwriting\n",
    "        build_dataset_page(dataset_id,dataset_title)\n",
    "\n",
    "\n",
    "def build_tables_from_csv(\n",
    "  data, #pandas dataset\n",
    "  max_rows: int, \n",
    "  dataset_id: str, #inner id for the dataset, eg. \"chiralMaps\"\n",
    "  dataset_title: str #actual title (on the website) eg \"Chiral maps up to 6000 edges\"\n",
    " ) -> None :\n",
    "    data_split_list=[]\n",
    "    for i in range(len(data)//max_rows):\n",
    "        data_split_list.append(data.iloc[max_rows*i:max_rows*(i+1)])\n",
    "    data_split_list.append(data.iloc[max_rows*(len(data)//max_rows):])\n",
    "\n",
    "    for i in range(len(data_split_list)):\n",
    "        frontmatter=f'---\\nlayout: default\\ndataset: {dataset_id}\\ndataset_title: {dataset_title}\\nfirst_entry: {data_split_list[i].iat[0,0]}\\nlast_entry: {data_split_list[i].iat[-1,0]}\\n---\\n\\n'\n",
    "        html=data_split_list[i].to_html(index=False)\n",
    "        html=html.replace('border=\"1\"', 'id=\"myTable\"')\n",
    "        html=html.replace('class=\"dataframe\"','class=\"display compact\" style=\"width=100%\"')\n",
    "        html=html.replace('style=\"text-align: right;\"','')\n",
    "        with open(f'../_tables/{dataset_id}{i}.md', 'w') as md_file:\n",
    "            md_file.write(frontmatter)\n",
    "            md_file.write(f\"The following table contains the entries from {{{{ page.first_entry }}}} to {{{{ page.last_entry }}}} of the dataset of [{{{{ page.dataset_title }}}}]( /datasets/{dataset_id} ).\\n \")\n",
    "            md_file.write(html)\n",
    "\n",
    "def build_dataset_page(\n",
    "  dataset_id: str, #inner id for the dataset, eg. \"chiralMaps\"\n",
    "  dataset_title: str #actual title (on the website) eg \"Chiral maps up to 6000 edges\"\n",
    ") -> None:\n",
    "    with open(f'../_datasets/{dataset_id}.md', 'w') as dataset_md:             \n",
    "        md_text=f\"--- \\nlayout: page\\ntitle: {dataset_title}\\n---\\n\"\n",
    "        md_text=md_text+f\"### Tables \\n<ol>\\n{{% for post in site.tables %}}\\n  {{% if post.dataset == '{dataset_id}' %}}\\n <li> A <a href= \\\"{{{{ site.url }}}}{{{{ post.url | relative_url }}}}\\\" > table </a> containing the entries from {{{{ post.first_entry }}}} to {{{{ post.last_entry }}}} </li>\\n{{% endif %}}{{% endfor %}} \\n </ol>\\n\\n\\n### Resources\"\n",
    "        dataset_md.write(md_text)\n",
    "\n",
    "def populate_a_dataEntry_page(\n",
    "    dataEntry, #an entry o pandas\n",
    "    toHumanDict: Dict, #a dictionary with the columns of the dataset translated to readable language\n",
    "    dataset_id: str, #the id of the dataset which it belongs\n",
    "    dataset_title: str #actual title (on the website) eg \"Chiral maps up to 6000 edges\"\n",
    ") -> None:\n",
    "    dataDict=dataEntry.to_dict()\n",
    "    frontmatter=f'--- \\n permalink: /{dataset_id}/{dataEntry.ID_url} \\n collection: {dataset_id}\\n layout: dataEntry\\n title: {dataset_title} - {dataEntry.ID}\\n---\\n\\n'\n",
    "    with open(f'../_{dataset_id}/{dataEntry.ID_url}.md', 'w') as md_file:\n",
    "        md_file.write(frontmatter)\n",
    "        #iterate over the entries of the dictionary and populate\n",
    "        for k in toHumanDict:\n",
    "            if len(toHumanDict[k])>1 and not isinstance(toHumanDict[k], str):\n",
    "                md_file.write(\"- **\"+toHumanDict[k][0]+\"**: [\"+str(dataDict[k])+\"]({{ site.url }}/\"+toHumanDict[k][1]+ str(dataDict[k+\"_url\"])+\")\\n\")\n",
    "            else:\n",
    "                md_file.write(\"- **\"+toHumanDict[k]+\"**: \"+str(dataDict[k])+\"\\n\")\n",
    "\n",
    "def create_dataPages_for_Dataset(\n",
    "    dataset_id:str, #dataset_id\n",
    "    dataSet, #a pandas Dataset\n",
    "    dataset_title: str, #actual title (on the website) eg \"Chiral maps up to 6000 edges\"\n",
    "    toHumanDict: Dict, #a dictionary with the columns of the dataset translated to readable language\n",
    ") -> None:\n",
    "  if not os.path.exists(f'../_{dataset_id}'):\n",
    "        os.mkdir(f'../_{dataset_id}')\n",
    "  for index,data in dataSet.iterrows():\n",
    "      populate_a_dataEntry_page(data,toHumanDict,dataset_id,dataset_title)\n",
    "\n",
    "def preprocess_DataSet(\n",
    "    dataset, #a pandas dataset\n",
    "    cols_to_url: List, #columns that need to be url-ised\n",
    ")->None:\n",
    "    dataset.replace(np.nan, '', regex=True)\n",
    "    for col in cols_to_url:\n",
    "        if col in dataset.columns:\n",
    "            dataset[col+\"_url\"]=dataset[col].str.replace(\"[\",\"_\")\n",
    "            dataset[col+\"_url\"]=dataset[col+\"_url\"].str.replace(\";\",\"_\")\n",
    "            dataset[col+\"_url\"]=dataset[col+\"_url\"].str.replace(\"]\",\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We need the following ingredients to populate the website:\n",
    "  - A **csv file** with the data that we want to use\n",
    "  - A string **dataset_id**  that will identify our dataset  internally, in particular this string will be used to build the links of the website, so it should be short and without spaces (more on this later). \n",
    "  A good example of this string would be:  \"_chiralMaps_\"\n",
    "  - A string **dataset_title** this is a readable string that will identify the dataset on the website, this is for humans to read. Eg. \"_Chiral maps up to 6000 edges_\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", the main page of the will be `https://anteromontonio.github.io/datasetsLj/dataset_id/` the tables pages will be `https://anteromontonio.github.io/datasetsLj/dataset_id_tab_i` and the pages for a given data entry of a given dataset will be `https://anteromontonio.github.io/datasetsLj/dataset_id/entry_id`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
