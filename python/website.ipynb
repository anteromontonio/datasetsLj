{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to cook a dataset website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an interactive python notebook used to populate the datasets wesbsite (https://anteromontonio.github.io/datasetsLj/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The kitchen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to import the python functions, we do this by loading the file ``` build_datasets```.\n",
    "Please, only modify this file if you know what you are doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load build_datasets.py\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#this is a change\n",
    "\n",
    "# This is kind of the main routine, it is used to build a dataset website from a csvfile\n",
    "# def build_pages_for_dataset(\n",
    "#         csv_file: str, #name of the csv_file without extension\n",
    "#         dataset_id: str, #inner id for the dataset, eg. \"chiralMaps\"\n",
    "#         dataset_title: str, #actual title to be displayed on the website, eg. \"Chiral maps up to 6000 edges\"\n",
    "#         max_rows :int =10000,\n",
    "# ) -> None :\n",
    "    \n",
    "#     dataSet=pd.read_csv(f\"../csv/{csv_file}.csv\")#reads csv file\n",
    "#     preprocess_DataSet(dataSet)\n",
    "    \n",
    "#     if not os.path.exists(f'../{dataset_id}'):\n",
    "#         os.mkdir(f'../{dataset_id}')\n",
    "\n",
    "#     build_tables_from_csv(data,max_rows,dataset_id,dataset_title)\n",
    "#     if not os.path.exists(f'../_datasets/{dataset_id}.md'): #only runs if the file does not exists, to avoid overwriting\n",
    "#         build_dataset_page(dataset_id,dataset_title)\n",
    "\n",
    "\n",
    "def build_tables_from_csv(\n",
    "  data, #pandas dataset\n",
    "  dataset_id: str, #inner id for the dataset, eg. \"chiralMaps\"\n",
    "  dataset_title: str, #actual title (on the website) eg \"Chiral maps up to 6000 edges\"\n",
    "  toShowInTables: List,\n",
    "  max_rows: int = 10000, \n",
    "  overwrite:bool = False\n",
    " ) -> None :\n",
    "    data_split_list=[]\n",
    "    for i in range(len(data)//max_rows):\n",
    "        data_split_list.append(data.iloc[max_rows*i:max_rows*(i+1)])\n",
    "    data_split_list.append(data.iloc[max_rows*(len(data)//max_rows):])\n",
    "    print(f'We are producing {len(data_split_list)} tables')\n",
    "    for i in range(len(data_split_list)):\n",
    "        frontmatter=f'---\\nlayout: default\\ndataset: {dataset_id}\\ndataset_title: {dataset_title}\\npermalink: /{dataset_id}/{dataset_id}_tab_{i} \\nfirst_entry: {data_split_list[i].iat[0,0]}\\nlast_entry: {data_split_list[i].iat[-1,0]}\\n---\\n\\n'\n",
    "        html=data_split_list[i].to_html(index=False, columns=toShowInTables,)\n",
    "        html=html.replace('border=\"1\"', 'id=\"myTable\"')\n",
    "        html=html.replace('class=\"dataframe\"','class=\"display compact\" style=\"width=100%\"')\n",
    "        html=html.replace('style=\"text-align: right;\"','')\n",
    "        # if not os.path.exists(f'../_tables/{dataset_id}_tab_{i}.md') or overwrite:\n",
    "        with open(f'../_tables/{dataset_id}_tab_{i}.md', 'w') as md_file:\n",
    "            md_file.write(frontmatter)\n",
    "                \n",
    "            preample=f\"The following table contains the entries from {{{{ page.first_entry }}}} to {{{{ page.last_entry }}}} of the dataset of [{{{{ page.dataset_title }}}}]( /datasets/{dataset_id} ).\\n You can use the buttons to toggle the view of the corresponding column. \\n\"\n",
    "\n",
    "\n",
    "            md_file.write(preample)\n",
    "\n",
    "\n",
    "            md_file.write(html)\n",
    "        print(f'Processing {dataset_id}_tab_{i}')\n",
    "        add_links_to_tables(f'{dataset_id}_tab_{i}',data_split_list[i])\n",
    "        print(f'{dataset_id}_tab_{i} Processed')\n",
    "\n",
    "def build_dataset_page(\n",
    "  dataset_id: str, #inner id for the dataset, eg. \"chiralMaps\"\n",
    "  dataset_title: str, #actual title (on the website) eg \"Chiral maps up to 6000 edges\"\n",
    "  toHumanDict: Dict, #A dictionary translating the information to human readable information.\n",
    "  res:str = \"\", #you can add a string with resources (link to gap packages, documentation, etc)... \n",
    "  overwrite:bool = False, # to force overwriting\n",
    ") -> None:\n",
    "    if not os.path.exists(f'../_datasets/{dataset_id}.md') or overwrite:\n",
    "        print(\"Overwriting\")\n",
    "        with open(f'../_datasets/{dataset_id}.md', 'w') as dataset_md:             \n",
    "            frontmatter=f\"--- \\n layout: page\\n title: {dataset_title}\\n permalink: /{dataset_id}/\\n---\\n\\n\"\n",
    "            \n",
    "            tables=f\"### Tables \\n<ol>\\n{{% for post in site.tables %}}\\n  {{% if post.dataset == '{dataset_id}' %}}\\n <li> <a href= \\\"{{{{ site.url }}}}{{{{ post.url | relative_url }}}}\\\" > Table </a> containing the entries from {{{{ post.first_entry }}}} to {{{{ post.last_entry }}}} </li>\\n{{% endif %}}{{% endfor %}} \\n </ol>\\n\\n\"\n",
    "\n",
    "            csvPage=f\"- You can download a csv file containing the data on this dataset [here]({{{{ site.url }}}}/csv/{dataset_id}.csv). \\n\"\n",
    "            resources=\"### Resources\\n\" + csvPage+res + \"\\n\\n\"\n",
    "\n",
    "            dictContents=\"This dataset contains (at least) the following information for every entry:\\n\"\n",
    "\n",
    "            for k in toHumanDict:\n",
    "                if len(toHumanDict[k])>1 and not isinstance(toHumanDict[k], str):\n",
    "                    dictContents= dictContents+\"- **\"+ k +\"**: \" + toHumanDict[k][0]+\"\\n\"\n",
    "                else:\n",
    "                    dictContents= dictContents+\"- **\"+ k +\"**: \" + toHumanDict[k]+\"\\n\"\n",
    "\n",
    "            md_text=frontmatter + \"\\n\"+dictContents + \"\\n\" + resources +\"\\n\"+ tables\n",
    "            dataset_md.write(md_text)\n",
    "#end of build dataset page\n",
    "\n",
    "def populate_a_dataEntry_page(\n",
    "    dataEntry, #an entry o pandas\n",
    "    toHumanDict: Dict, #a dictionary with the columns of the dataset translated to readable language\n",
    "    dataset_id: str, #the id of the dataset which it belongs\n",
    "    dataset_title: str #actual title (on the website) eg \"Chiral maps up to 6000 edges\"\n",
    ") -> None:\n",
    "    dataDict=dataEntry.to_dict()\n",
    "    frontmatter=f'--- \\n permalink: /{dataset_id}/{dataEntry.ID_url} \\n collection: {dataset_id}\\n layout: dataEntry\\n title: {dataset_title} - {dataEntry.ID}\\n---\\n\\n'\n",
    "    with open(f'../_{dataset_id}/{dataEntry.ID_url}.md', 'w') as md_file:\n",
    "        md_file.write(frontmatter)\n",
    "        #iterate over the entries of the dictionary and populate\n",
    "        for k in toHumanDict:\n",
    "            if len(toHumanDict[k])>1 and not isinstance(toHumanDict[k], str):\n",
    "                md_file.write(\"- **\"+toHumanDict[k][0]+\"**: [\"+str(dataDict[k])+\"]({{ site.url }}/\"+toHumanDict[k][1]+ str(dataDict[k+\"_url\"])+\")\\n\")\n",
    "            else:\n",
    "                md_file.write(\"- **\"+toHumanDict[k]+\"**: \"+str(dataDict[k])+\"\\n\")\n",
    "\n",
    "def create_dataPages_for_Dataset(\n",
    "    dataset_id:str, #dataset_id\n",
    "    dataSet, #a pandas Dataset\n",
    "    dataset_title: str, #actual title (on the website) eg \"Chiral maps up to 6000 edges\"\n",
    "    toHumanDict: Dict, #a dictionary with the columns of the dataset translated to readable language\n",
    ") -> None:\n",
    "  if not os.path.exists(f'../_{dataset_id}'):\n",
    "        os.mkdir(f'../_{dataset_id}')\n",
    "  for index,data in dataSet.iterrows():\n",
    "      populate_a_dataEntry_page(data,toHumanDict,dataset_id,dataset_title)\n",
    "\n",
    "def preprocess_DataSet(\n",
    "    dataset, #a pandas dataset\n",
    "    cols_to_url: List, #columns that need to be url-ised\n",
    ")->None:\n",
    "    dataset.replace(np.nan, '', regex=True)\n",
    "    for col in cols_to_url:\n",
    "        if col in dataset.columns:\n",
    "            dataset[col+\"_url\"]=dataset[col].str.replace(\"[\",\"_\")\n",
    "            dataset[col+\"_url\"]=dataset[col+\"_url\"].str.replace(\";\",\"_\")\n",
    "            dataset[col+\"_url\"]=dataset[col+\"_url\"].str.replace(\"]\",\"\")\n",
    "\n",
    "#slow, only alows links withing the same dataset\n",
    "def add_links_to_tables(\n",
    "    table_file:str, #path to the table file\n",
    "    dataset, #pandas dataset with url-s columns\n",
    ") -> None:\n",
    "    \n",
    "    with open(f'../_tables/{table_file}.md', 'r') as tableF:\n",
    "        contents=tableF.read()\n",
    "        for index,row in dataset.iterrows():\n",
    "            link_str=f'<a href=\"./{str(row.ID_url)}.html\"> {str(row.ID)}</a>'\n",
    "            contents=contents.replace(str(row.ID),link_str)\n",
    "    with open(f'../_tables/{table_file}.md', 'w') as tableF:\n",
    "       tableF.write(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load toHumanDicts.py\n",
    "toHumanDicts={\n",
    "\"chiralMaps6kE\":{'ID':['ID',\"test/\"], 'genus':'Genus of the underlying surface', 'p':'Length of the face', 'q':'Valency', 'r':'Length of the Petrie polygon', 'solv':'Is the automorphism group solvable?', 'V':'Number of vertices', 'E':'Number of edges', 'F':'Number of faces', 'self':'The map is self-', 'Du':['Dual map','test/'], 'Mir':['Mirror (enantihomorphic) map',\"test/\"], 'DuMir':['Dual of the mirror image',\"test/\"], 'Zq*:Exp':'Z_q-Exponent?', 'Hj':'No idea', 'plt':'No idea', 'plh':'No idea', 'Sk':'Skeleton', 'PlhSk':'Some other kind of skeleton' },\n",
    "\"test\":{'ID':['ID',\"test/\"], 'genus':'Genus of the underlying surface', 'p':'Length of the face', 'q':'Valency', 'r':'Length of the Petrie polygon', 'solv':'Is the automorphism group solvable?', 'V':'Number of vertices', 'E':'Number of edges', 'F':'Number of faces', 'vMult':'Vertex multiplicity', 'fMult':'Face multiplicity', 'self':'The map is self-', 'Du':['Dual map','test/'], 'Mir':['Mirror (enantihomorphic) map',\"test/\"], 'DuMir':['Dual of the mirror image',\"test/\"], 'Zq*:Exp':'Z_q-Exponent?', 'Hj':'No idea', 'plt':'No idea', 'plh':'No idea', 'Sk':'Skeleton', 'PlhSk':'Some other kind of skeleton' },\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the following ingredients to populate the website:\n",
    "  1. A string **csv_file** with the data that we want to use\n",
    "  1. A string **dataset_id**  that will identify our dataset  internally, in particular this string will be used to build the links of the website, so it should be short and without spaces (more on this later). \n",
    "  A good example of this string would be:  \"_chiralMaps_\"\n",
    "  3. A string **dataset_title** this is a readable string that will identify the dataset on the website, this is for humans to read. Eg. \"_Chiral maps up to 6000 edges_\".\n",
    "  4. A python dictionary **toHumanDict** that translates the columns of the csv file into readable language. This will be used to build the individual data pages.\n",
    "  More on this dictionary later. Example:\n",
    "  ```\n",
    "  chiralMaps_Dict={'ID':'ID',\n",
    "          'genus':'Genus of the underlying surface',\n",
    "          'p':'Length of the face',\n",
    "          'q':'Valency',\n",
    "          'r':'Length of the Petrie polygon',\n",
    "          'solv':'Is the automorphism group solvable?',\n",
    "          'V':'Number of vertices',\n",
    "          'E':'Number of edges',\n",
    "          'F':'Number of faces',\n",
    "          'vMult':'Vertex multiplicity',\n",
    "          'fMult':'Face multiplicity',\n",
    "          'self':'The map is self-',\n",
    "          'Du':['Dual map','test/'],\n",
    "          'Mir':['Mirror (enantihomorphic) map',\"test/\"],\n",
    "          'DuMir':['Dual of the mirror image',\"test/\"],\n",
    "          'Zq*:Exp':'Z_q-Exponent?',\n",
    "          'Hj':'No idea',\n",
    "          'plt':'No idea',\n",
    "          'plh':'No idea',\n",
    "          'Sk':'Skeleton',\n",
    "          'PlhSk':'Some other kind of skeleton'\n",
    "          }\n",
    "\n",
    "  ```\n",
    "  5. (Optional) A list **toShowInTables** containing some of the keys of **ToHumanDict**. The columns in this list will be the ones shown on the tables, the default is all of them.\n",
    "  6. (Optional) A list **withLinks** containing those keys of **toHumanDict** \n",
    "\n",
    "You can use the following cell to define these ingredients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file=\"chiralMaps6kE\"\n",
    "dataset_id=\"chiralMaps6kE\"\n",
    "dataset_title=\"Chiral maps up to 6000 edges\"\n",
    "toHumanDict=toHumanDicts[dataset_id] #we are using the auxiliary file toHumanDicts.py, make sure that you load it and that it contains the appropriate dictionary\n",
    "toShowInTables=[k for k in toHumanDict] #all the keys by default\n",
    "withLinks=[k for k in toHumanDict if (len(toHumanDict[k])>1 and not isinstance(toHumanDict[k], str))] #those keys should be given implicitly with the dictionary, more on this below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cooking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We first turn the csv file into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5c/_0tgkjqd0xxgkybrf986ybm80000gn/T/ipykernel_6767/2675032700.py:1: DtypeWarning: Columns (19,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dataSet=pd.read_csv(f\"../csv/{csv_file}.csv\")#reads csv file\n"
     ]
    }
   ],
   "source": [
    "dataSet=pd.read_csv(f\"../csv/{csv_file}.csv\")#reads csv file\n",
    "preprocess_DataSet(dataSet,withLinks)\n",
    "#dataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We now build the main page of the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting\n"
     ]
    }
   ],
   "source": [
    "build_dataset_page(dataset_id,dataset_title,toHumanDict,)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We build the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are producing 47 tables\n",
      "Processing chiralMaps6kE_tab_0\n",
      "chiralMaps6kE_tab_0 Processed\n",
      "Processing chiralMaps6kE_tab_1\n",
      "chiralMaps6kE_tab_1 Processed\n",
      "Processing chiralMaps6kE_tab_2\n",
      "chiralMaps6kE_tab_2 Processed\n",
      "Processing chiralMaps6kE_tab_3\n",
      "chiralMaps6kE_tab_3 Processed\n",
      "Processing chiralMaps6kE_tab_4\n",
      "chiralMaps6kE_tab_4 Processed\n",
      "Processing chiralMaps6kE_tab_5\n",
      "chiralMaps6kE_tab_5 Processed\n",
      "Processing chiralMaps6kE_tab_6\n",
      "chiralMaps6kE_tab_6 Processed\n",
      "Processing chiralMaps6kE_tab_7\n",
      "chiralMaps6kE_tab_7 Processed\n",
      "Processing chiralMaps6kE_tab_8\n",
      "chiralMaps6kE_tab_8 Processed\n",
      "Processing chiralMaps6kE_tab_9\n",
      "chiralMaps6kE_tab_9 Processed\n",
      "Processing chiralMaps6kE_tab_10\n",
      "chiralMaps6kE_tab_10 Processed\n",
      "Processing chiralMaps6kE_tab_11\n",
      "chiralMaps6kE_tab_11 Processed\n",
      "Processing chiralMaps6kE_tab_12\n",
      "chiralMaps6kE_tab_12 Processed\n",
      "Processing chiralMaps6kE_tab_13\n",
      "chiralMaps6kE_tab_13 Processed\n",
      "Processing chiralMaps6kE_tab_14\n",
      "chiralMaps6kE_tab_14 Processed\n",
      "Processing chiralMaps6kE_tab_15\n",
      "chiralMaps6kE_tab_15 Processed\n",
      "Processing chiralMaps6kE_tab_16\n",
      "chiralMaps6kE_tab_16 Processed\n",
      "Processing chiralMaps6kE_tab_17\n",
      "chiralMaps6kE_tab_17 Processed\n",
      "Processing chiralMaps6kE_tab_18\n",
      "chiralMaps6kE_tab_18 Processed\n",
      "Processing chiralMaps6kE_tab_19\n",
      "chiralMaps6kE_tab_19 Processed\n",
      "Processing chiralMaps6kE_tab_20\n",
      "chiralMaps6kE_tab_20 Processed\n",
      "Processing chiralMaps6kE_tab_21\n",
      "chiralMaps6kE_tab_21 Processed\n",
      "Processing chiralMaps6kE_tab_22\n",
      "chiralMaps6kE_tab_22 Processed\n",
      "Processing chiralMaps6kE_tab_23\n",
      "chiralMaps6kE_tab_23 Processed\n",
      "Processing chiralMaps6kE_tab_24\n",
      "chiralMaps6kE_tab_24 Processed\n",
      "Processing chiralMaps6kE_tab_25\n",
      "chiralMaps6kE_tab_25 Processed\n",
      "Processing chiralMaps6kE_tab_26\n",
      "chiralMaps6kE_tab_26 Processed\n",
      "Processing chiralMaps6kE_tab_27\n",
      "chiralMaps6kE_tab_27 Processed\n",
      "Processing chiralMaps6kE_tab_28\n",
      "chiralMaps6kE_tab_28 Processed\n",
      "Processing chiralMaps6kE_tab_29\n",
      "chiralMaps6kE_tab_29 Processed\n",
      "Processing chiralMaps6kE_tab_30\n",
      "chiralMaps6kE_tab_30 Processed\n",
      "Processing chiralMaps6kE_tab_31\n",
      "chiralMaps6kE_tab_31 Processed\n",
      "Processing chiralMaps6kE_tab_32\n",
      "chiralMaps6kE_tab_32 Processed\n",
      "Processing chiralMaps6kE_tab_33\n",
      "chiralMaps6kE_tab_33 Processed\n",
      "Processing chiralMaps6kE_tab_34\n",
      "chiralMaps6kE_tab_34 Processed\n",
      "Processing chiralMaps6kE_tab_35\n",
      "chiralMaps6kE_tab_35 Processed\n",
      "Processing chiralMaps6kE_tab_36\n",
      "chiralMaps6kE_tab_36 Processed\n",
      "Processing chiralMaps6kE_tab_37\n",
      "chiralMaps6kE_tab_37 Processed\n",
      "Processing chiralMaps6kE_tab_38\n",
      "chiralMaps6kE_tab_38 Processed\n",
      "Processing chiralMaps6kE_tab_39\n",
      "chiralMaps6kE_tab_39 Processed\n",
      "Processing chiralMaps6kE_tab_40\n",
      "chiralMaps6kE_tab_40 Processed\n",
      "Processing chiralMaps6kE_tab_41\n",
      "chiralMaps6kE_tab_41 Processed\n",
      "Processing chiralMaps6kE_tab_42\n",
      "chiralMaps6kE_tab_42 Processed\n",
      "Processing chiralMaps6kE_tab_43\n",
      "chiralMaps6kE_tab_43 Processed\n",
      "Processing chiralMaps6kE_tab_44\n",
      "chiralMaps6kE_tab_44 Processed\n",
      "Processing chiralMaps6kE_tab_45\n",
      "chiralMaps6kE_tab_45 Processed\n",
      "Processing chiralMaps6kE_tab_46\n",
      "chiralMaps6kE_tab_46 Processed\n"
     ]
    }
   ],
   "source": [
    "build_tables_from_csv(dataSet,dataset_id,dataset_title,toShowInTables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We build the individual data pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataPages_for_Dataset(dataset_id,dataSet,dataset_title,toHumanDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
